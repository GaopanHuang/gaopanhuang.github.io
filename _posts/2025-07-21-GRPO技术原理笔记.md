---
layout: post
title: GRPO技术原理笔记
categories: 模型后训练
description: 对DeepSeek提出的GRPO做笔记
keywords: llm, GRPO, post-training
---

原始论文-2024.4：[DeepSeekMath- Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300)


# GRPO整体流程
## 与PPO的对比

<img src="/images/posts/post-training/grpoflow.png" width="80%" alt="GRPO流程图"/>

## GRPO算法流程

<img src="/images/posts/post-training/grpoalgo.png" width="80%" alt="GRPO算法流程"/>

1. **为每个问题生成多个回答**
    
    对于训练集中的每个问题或提示 sj，使用当前策略模型 Πθold 生成多个回答
    
    在 DeepSeekMath 的实践中，每个问题通常生成 64 个样本，最大长度为 1024 个 tokens。
    
    生成过程通常使用温度采样（temperature sampling），以确保回答的多样性。
    
2. **使用奖励模型评分**
    
    每个生成的回答 oi 都会通过奖励模型获得一个分数 ri。奖励模型可以是：
    
    - 规则型奖励：例如，对于数学问题，可以根据最终答案的正确性给出二元奖励（正确为 1，错误为 0）。
    - 学习型奖励：使用单独训练的模型或者使用其他的 LLM 作为裁判来评估回答质量，这在没有明确标准答案的任务中有用。
    - 混合奖励：结合多种信号，如答案正确性、解题过程的清晰度、格式规范性等。
    
    在 DeepSeek-R1 的训练中，奖励模型使用的是规则型奖励，不仅包括答案的准确性，也考虑了回答的格式质量。
    
3. **计算群组平均奖励**
    
    对于每个问题 sj，计算其所有回答的平均奖励，这个平均值将作为估计优势值的基线。
    
4. **相对优势计算**
    
    计算每个回答的相对优势值，并进行标准化以减少方差。
    
    这一步的核心思想是：对于同一个问题，那些获得高于平均奖励的回答应该被强化，而那些低于平均值的回答则应该被抑制。
    
5. **策略更新**
    
    基于计算出的优势值和目标函数，使用梯度下降法更新策略网络的参数。
    
    在实践中，通常使用小批量梯度下降，并控制每次更新的幅度确保训练稳定。DeepSeek-R1 采用的学习率为 1e-6。
    

除了基本算法外，GRPO 的成功实现还依赖于一些关键技巧：

1. 批量大小与更新频率
    
    DeepSeek-R1 使用 1024 的批量大小，并在每个探索阶段仅进行一次更新，这有助于保持训练稳定性。
    
2. KL 散度系数调节
    
    KL 散度系数 β 的选择至关重要：太小会导致模型偏离过快，丧失通用能力；太大则会使模型更新过于保守，学习效率低下。
    
3. 回答长度控制
    
    在生成多个回答时，需要控制最大生成长度，在 DeepSeek 的实践中通常设置为 1024 个 tokens，这既保证了足够的推理空间，又避免了过长序列带来的计算负担。
    
4. 奖励标准化
    
    除了群组内标准化外，有时还会进行全局奖励标准化，即根据所有问题的奖励分布来标准化每个奖励，这有助于处理不同问题难度差异带来的奖励分布不均匀问题。
    
5. 参考策略选择
    
    KL 散度正则化中的参考策略 Πθref 通常选择初始的 SFT 模型，这有助于保持模型的通用能力，防止过度专注于特定任务而忽略基础能力。

ref：https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/

## Iterative RL with GRPO

在原始的GRPO中，reward model是基于模型来实现的，不是rule-based，因此原文提出迭代强化学习（Iterative RL）的概念，即GRPO算法流程Algorithm 1 line 12：*Update 𝑟𝜑 through continuous training using a replay mechanism.*

随着强化学习训练过程的推进，旧的reward model可能不足以监督当前的policy model。因此探索了基于 GRPO 的迭代强化学习。在迭代 GRPO 中，根据policy model的采样结果生成新的reward model训练集，并使用一个回放机制持续训练旧的reward model，该机制包含 10% 的历史数据。然后，将ref model设置为policy model (𝜋𝑟𝑒𝑓 ← 𝜋-*theta*)，并使用新的reward model持续训练policy model。


# EasyR1的GRPO实现

<img src="/images/posts/post-training/easyr1-grpo.png" width="80%" alt="EasyR1 的 GRPO实现流程"/>

以 GRPO 算法为例，EasyR1 在进行训练时，分为以下几个主要步骤：

- 轨迹采样（左上）：从数据中选取批量大小（batch-size）个问题，由当前的策略模型对每个问题生成多个答案，作为当前策略模型的推理轨迹。
- 计算策略概率（右上）：将所有的推理轨迹输入到策略模型中，计算当前策略模型选择这些轨迹的概率分布。
- 计算参考概率（右下）：再将上述的所有推理轨迹输入到参考模型中，计算参考模型选择这些轨迹的概率分布，用于计算 KL 散度。如果 KL 散度被关闭，则该步跳过。
- 计算奖励和优势函数：基于预先设置的奖励模型或者规则验证器，对所有的模型回答计算奖励分数，再按照群组归一化方式得到优势函数。
- 更新策略模型（左下）：根据问答对的策略概率和优势函数，分小批量（mini-batch）更新当前策略模型的参数。
- 将更新后的策略模型作为新的模型，对新的一批数据进行下一轮的轨迹采样（左上），不断循环这一过程。

其中计算轨迹概率（右上或右下）伪代码如下：

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# 加载 tokenizer 和 模型
model_name = "/path/to/Qwen-7B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# 输入文本
input_text = "Hello, world"

# Tokenize 输入文本（不添加特殊token）
input_ids = tokenizer(input_text, return_tensors="pt").input_ids
attention_mask = inputs["attention_mask"]

# 前向传播（logits）
with torch.no_grad():
    outputs = model(input_ids=input_ids)
    logits = outputs.logits  # shape: (batch_size, seq_len, vocab_size)

# 计算 log softmax 得到 log probabilities
log_probs = torch.nn.functional.log_softmax(logits, dim=-1)

# 获取每个 token 的 log probs
input_ids = input_ids[0]  # 去掉 batch 维度 (seq_len,)
log_probs = log_probs[0]  # 同样去掉 batch 维度 (seq_len, vocab_size)

# 打印每个 token 及其对应的 log probability
total_log_prob = 0.0
print(f"{'Token':<15} | {'Log Prob'}")
print("-" * 25)
for i in range(len(input_ids)):
    token_id = input_ids[i]
    token = tokenizer.decode([token_id])
    log_prob = log_probs[i][token_id].item()
    total_log_prob += log_prob
    print(f"{token:<15} | {log_prob:.4f}")

print("\nTotal log probability of 'Hello, world': {:.4f}".format(total_log_prob))
```

ref：https://github.com/hiyouga/EasyR1

# swift的GRPO实现

https://swift.readthedocs.io/zh-cn/latest/Instruction/GRPO/GetStarted/GRPO.html

```python
# ========== 1. Rollout Generation Phase ==========
prompt = "Question: Which is bigger? 9.11 or 9.9?"

# Generate multiple completions through parallel sampling
completions = rollout_function(
    model=current_policy_model,
    prompt=prompt,
    num_generations=8,  # Hyperparameter: number of samples per prompt
    temperature=1.0     # Hyperparameter: sampling diversity
)
"""
completions = [
    (completion 1) "The larger number is 9.11...",
    (completion 2) "9.9 is bigger than...",
    ...
    (completion 8) "After calculation, 9.11..."
]
"""

# ========== 2. Reward Calculation Phase ==========
# Evaluate generated completions using reward model
rewards = reward_function(
    completions=completions,
    ground_truth="9.11"  # Expected correct answer
)
"""
rewards = [
    (reward 1) 1.0,  # Correct answer
    (reward 2) 0.0,  # Incorrect
    ...
    (reward 8) 1.0   # Correct
]
"""

# Normalize rewards to advantages
rewards_mean = mean(rewards)  # μ = 0.5
rewards_std = std(rewards)    # σ = 0.25
advantages = (rewards - rewards_mean) / (rewards_std + 1e-8)  # Standardization
"""
advantages = [
    (advantage 1)  2.0,  # (1.0 - 0.5)/0.25
    (advantage 2) -2.0,
    ...
    (advantage 8)  2.0
]
"""

# ========== 3. Policy Optimization Phase ==========
# Get token-level log probabilities from different models
current_logps = get_per_token_logps(current_policy_model, prompt, completions)  # π_θ
old_logps = get_per_token_logps(old_policy_model, prompt, completions)          # π_θ_old
ref_logps = get_per_token_logps(reference_model, prompt, completions)           # π_ref

# PPO Clipped Objective
is_ratio = exp(current_logps - old_logps)  # Importance sampling ratio: e^(π_θ - π_θ_old)
clipped_ratio = clip(is_ratio, 1-ε, 1+ε)   # ε=0.2 typically

# Policy gradient term (dual form)
policy_loss = -mean(
    minimum(is_ratio * advantages,       # Unclipped objective
           clipped_ratio * advantages)  # Clipped objective
)

# KL Divergence Penalty (K3 estimator)
# KL(π_θ||π_ref) ≈ e^(logπ_ref - logπ_θ) - (logπ_ref - logπ_θ) - 1
kl_penalty = beta * mean(
    exp(ref_logps - current_logps) -
    (ref_logps - current_logps) - 1
)

# Total Loss = Policy Loss + KL Penalty
total_loss = policy_loss + kl_penalty

# ========== 4. Update Rule ==========
# Apply gradient descent to minimize total_loss
optimizer.zero_grad()
total_loss.backward()
optimizer.step()
```

# 统一分析模型训练方法的一种范式

<img src="/images/posts/post-training/Unified Paradigm.png" width="80%" alt="强化学习的统一范式"/>

- **监督微调**（Supervised Fine-tuning, SFT）：SFT使用人类选择的SFT数据对预训练模型进行微调。
- **拒绝采样微调**（Rejection Sampling Fine-tuning, RFT）：RFT利用SFT数据的Query，对SFT模型的输出进行过滤，并通过过滤后的QA数据进一步微调SFT模型。RFT根据答案的正确性来过滤这些输出。
- **直接偏好优化**（Direct Preference Optimization, DPO）：DPO通过使用从SFT模型中采样的增强输出，并利用成对的DPO损失函数对该模型进行微调，从而进一步精炼SFT模型。
- **在线拒绝采样微调**（Online Rejection Sampling Fine-tuning, Online RFT）：与RFT不同，在线RFT首先使用SFT模型初始化策略模型，然后通过使用从实时策略模型中采样的增强输出对策略模型进行微调。
- **PPO/GRPO**：PPO/GRPO首先使用SFT模型初始化策略模型，然后通过使用从实时策略模型中采样的输出对其进行强化。Proximal Policy Optimization (PPO)

因此，RFT、DPO属于offline的强化学习，Online RFT、PPO、GRPO属于online的强化学习。
